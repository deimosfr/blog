<!doctype html><html lang=en-us><head><script type=application/ld+json>{"@context":"http://schema.org","@type":"Website","@id":"https:\/\/blog.deimos.fr","author":{"@type":"Person","name":"Pierre Mavro / Deimosfr","image":"https://www.gravatar.com/avatar/194382a6478d109ac45c9b11bad1945e"},"name":"Deimosfr Blog","description":"For my own company MySocialApp, I\x26rsquo;m managing multiple Cassandra clusters on top of a Kubernetes on premise cluster. For those who never heard of this distributed database, here is the summary from the official website:\n The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.","url":"https:\/\/blog.deimos.fr\/2018\/06\/24\/running-cassandra-on-kubernetes\/","keywords":"[]"}</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.69.2 with theme Tranquilpeak 0.5.2-BETA"><meta name=author content="Pierre Mavro / Deimosfr"><meta name=keywords content><meta name=description content="For my own company MySocialApp, I&rsquo;m managing multiple Cassandra clusters on top of a Kubernetes on premise cluster. For those who never heard of this distributed database, here is the summary from the official website:
 The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data."><meta property="og:description" content="For my own company MySocialApp, I&rsquo;m managing multiple Cassandra clusters on top of a Kubernetes on premise cluster. For those who never heard of this distributed database, here is the summary from the official website:
 The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data."><meta property="og:type" content="article"><meta property="og:title" content="Running Cassandra in Kubernetes"><meta name=twitter:title content="Running Cassandra in Kubernetes"><meta property="og:url" content="https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/"><meta property="twitter:url" content="https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/"><meta property="og:site_name" content="Deimosfr Blog"><meta property="og:description" content="For my own company MySocialApp, I&rsquo;m managing multiple Cassandra clusters on top of a Kubernetes on premise cluster. For those who never heard of this distributed database, here is the summary from the official website:
 The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data."><meta name=twitter:description content="For my own company MySocialApp, I&rsquo;m managing multiple Cassandra clusters on top of a Kubernetes on premise cluster. For those who never heard of this distributed database, here is the summary from the official website:
 The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data."><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2018-07-16T00:00:00"><meta property="article:modified_time" content="2018-07-16T00:00:00"><meta property="article:section" content="Kubernetes"><meta property="article:section" content="Containers"><meta property="article:section" content="Cassandra"><meta property="article:section" content="Databases"><meta property="article:section" content="Distributed"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Containers"><meta property="article:tag" content="Cassandra"><meta property="article:tag" content="Databases"><meta property="article:tag" content="Distributed"><meta name=twitter:card content="summary"><meta name=twitter:site content="@deimosfr"><meta name=twitter:creator content="@deimosfr"><meta property="og:image" content="https://www.gravatar.com/avatar/194382a6478d109ac45c9b11bad1945e?s=640"><meta property="twitter:image" content="https://www.gravatar.com/avatar/194382a6478d109ac45c9b11bad1945e?s=640"><meta property="og:image" content="https://blog.deimos.fr/thumbnails/logo_cassandra.png"><meta property="twitter:image" content="https://blog.deimos.fr/thumbnails/logo_cassandra.png"><title>Running Cassandra in Kubernetes</title><link rel=icon href=https://blog.deimos.fr/favicon.png><link rel=canonical href=https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://blog.deimos.fr/css/style-c5yjx2losbmkttdif4mskx5f9pqjjquywurgngxfvceyyrtyatseix5hpd.min.css><link rel=stylesheet href=https://blog.deimos.fr/css/custom.css><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-63927289-1','auto');ga('send','pageview');}</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://blog.deimos.fr/ aria-label="Go to homepage">Deimosfr Blog</a></div><a class=header-right-picture href=https://blog.deimos.fr/#about aria-label="Open the link: /#about"><img class=header-picture src="https://www.gravatar.com/avatar/194382a6478d109ac45c9b11bad1945e?s=90" alt="Author's picture"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://blog.deimos.fr/#about aria-label="Read more about the author"><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/194382a6478d109ac45c9b11bad1945e?s=110" alt="Author's picture"></a><h4 class=sidebar-profile-name>Pierre Mavro / Deimosfr</h4></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://blog.deimos.fr/ title=Home><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Home</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.deimos.fr/archives title=Archives><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Archives</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.deimos.fr/categories title=Categories><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Categories</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.deimos.fr/tags title=Tags><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Tags</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://blog.deimos.fr/index.xml title=RSS><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul><ul class=sidebar-buttons></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://www.deimos.fr target=_blank rel=noopener title=About><i class="sidebar-button-icon fa fa-lg fa-user"></i><span class=sidebar-button-desc>About</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://wiki.deimos.fr target=_blank rel=noopener title=Wiki><i class="sidebar-button-icon fa fa-wikipedia-w"></i><span class=sidebar-button-desc>Wiki</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post id=top><div class="post-header main-content-wrap text-left"><h1 class=post-title>Running Cassandra in Kubernetes</h1><div class="postShorten-meta post-meta"><time datetime=2018-07-16T00:00:00+02:00>16 July 2018</time>
<span>in</span>
<a class=category-link href=https://blog.deimos.fr/categories/kubernetes>Kubernetes</a>,
<a class=category-link href=https://blog.deimos.fr/categories/containers>Containers</a>,
<a class=category-link href=https://blog.deimos.fr/categories/cassandra>Cassandra</a>,
<a class=category-link href=https://blog.deimos.fr/categories/databases>Databases</a>,
<a class=category-link href=https://blog.deimos.fr/categories/distributed>Distributed</a></div></div><div class="post-content markdown"><div class=main-content-wrap><p><img src=https://blog.deimos.fr/images/logo_cassandra.png alt=Cassandra></p><p>For my own company <a href="https://mysocialapp.io/?ref=deimos_fr_running_cassandra_on_kubernetes">MySocialApp</a>, I&rsquo;m managing multiple <a href=http://cassandra.apache.org/>Cassandra</a> clusters on top of a Kubernetes on premise cluster. For those who never heard of this distributed database, here is the summary from the official website:</p><blockquote><p>The <a href=http://cassandra.apache.org/>Apache Cassandra</a> database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra&rsquo;s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages.</p></blockquote><p>What does that mean having a Cassandra cluster in a Kubernetes on premise cluster? Here are the basics:</p><ul><li><strong>Data location</strong>: having data means, you should be able to know where they are located (physically, or should be able to access them in a known way). You can&rsquo;t run a Cassandra instance anywhere if you&rsquo;re not sure the data will be there and if you care about your data (of course).</li><li><strong>Performance</strong>: distributed storage on top of a distributed database is strongly discouraged if you&rsquo;re searching for the best performances.</li><li><strong>Observability</strong>: distributed solutions are generaly more complex to observe. The observability tooling around it must be good enough to easily troubleshot.</li><li><strong>Self management</strong>: having a Cassandra self managed solution on Kubernetes is a good idea, however managing all scenario cases on distributed systems is a very long and complex achievement.</li></ul><p>That&rsquo;s why in this blog post, you&rsquo;ll see how I made a <a href=https://github.com/MySocialApp/kubernetes-helm-chart-cassandra/>Cassandra helm chart</a>, what are the benefits based on my experience on Cassandra + Kubernetes and finally how it works.</p><h1 id=performances>Performances</h1><p>Cassandra require a local storage to get the best performances. That&rsquo;s why you should avoid using distributed storage and use a hostPath or <a href=https://kubernetes.io/docs/concepts/storage/volumes/>localstorage</a> configuration like:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=k>name</span><span class=p>:</span><span class=w> </span>cassandra-data<span class=w>
</span><span class=w>    </span><span class=k>hostPath</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>path</span><span class=p>:</span><span class=w> </span>/mnt/data/{{<span class=w> </span>.Release.Namespace<span class=w> </span>}}/cassandra</code></pre></div><p>More than that, Cassandra maanges by itself replication. Let&rsquo;s say you&rsquo;ve got a RF (Replication Factor) of 3, your data is available on 3 nodes. If you&rsquo;re using a distributed storage on top of Cassandra, it certainly also has a x3 RF. Which mean the same <strong>data is stored 9 times</strong>. That&rsquo;s a huge waste of space and money in addition of the performances.</p><p>Another thing, I&rsquo;ve added some default Cassandra options, however I recommend to update the Cassandra configuration settings according to your hardware and wishes:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>cassandraConfig</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>concurrentWrites</span><span class=p>:</span><span class=w> </span><span class=m>128</span><span class=w>
</span><span class=w>  </span><span class=k>concurrentReads</span><span class=p>:</span><span class=w> </span><span class=m>128</span><span class=w>
</span><span class=w>  </span><span class=k>concurrentCompactors</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span><span class=w>  </span><span class=k>batchSizeWarnThresholdInKb</span><span class=p>:</span><span class=w> </span><span class=m>64</span><span class=w>
</span><span class=w>  </span><span class=k>batchSizeFailThresholdInKb</span><span class=p>:</span><span class=w> </span><span class=m>640</span><span class=w>
</span><span class=w>  </span><span class=k>compactionThroughputMbPerSec</span><span class=p>:</span><span class=w> </span><span class=m>150</span><span class=w>
</span><span class=w>  </span><span class=k>heapNewSize</span><span class=p>:</span><span class=w> </span>256M<span class=w>
</span><span class=w>  </span><span class=k>hintedHandoffThrottleInKb</span><span class=p>:</span><span class=w> </span><span class=m>4096</span><span class=w>
</span><span class=w>  </span><span class=k>maxHeap</span><span class=p>:</span><span class=w> </span>4G<span class=w>
</span><span class=w>  </span><span class=k>maxHintsDeliveryThreads</span><span class=p>:</span><span class=w> </span><span class=m>4</span><span class=w>
</span><span class=w>  </span><span class=k>memtableAllocationType</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;offheap_objects&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>memtableFlushWriter</span><span class=p>:</span><span class=w> </span><span class=m>4</span><span class=w>
</span><span class=w>  </span><span class=k>memtableCleanupThreshold</span><span class=p>:</span><span class=w> </span><span class=m>0.2</span><span class=w>
</span><span class=w>  </span><span class=k>rowCacheSizeInMb</span><span class=p>:</span><span class=w> </span><span class=m>128</span><span class=w>
</span><span class=w>  </span><span class=k>rowCacheSavePeriod</span><span class=p>:</span><span class=w> </span><span class=m>14400</span></code></pre></div><p>To reduce performance issues as well and reduce cluster failure in case of an issue, I should <strong>avoid getting multiple instances on the same physocal host</strong>. For that I&rsquo;m using <code>podAntiAffinity</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>affinity</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>podAntiAffinity</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>requiredDuringSchedulingIgnoredDuringExecution</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=k>labelSelector</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=k>matchExpressions</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=k>key</span><span class=p>:</span><span class=w> </span>app<span class=w>
</span><span class=w>          </span><span class=k>operator</span><span class=p>:</span><span class=w> </span>In<span class=w>
</span><span class=w>          </span><span class=k>values</span><span class=p>:</span><span class=w>
</span><span class=w>          </span>- {{<span class=w> </span>template<span class=w> </span><span class=s2>&#34;kubernetes.name&#34;</span><span class=w> </span>.<span class=w> </span>}}<span class=w>
</span><span class=w>      </span><span class=k>topologyKey</span><span class=p>:</span><span class=w> </span>kubernetes.io/hostname</code></pre></div><p>You can also tune the JVM memory allocation and parameters to optimize memory usage in a container.</p><h1 id=statefulset--self-management>Statefulset & self management</h1><h2 id=statefulset>Statefulset</h2><p>On this <a href=https://github.com/MySocialApp/kubernetes-helm-chart-cassandra/>Cassandra helm chart</a>, I decided to use <code>Statefulset</code> because:</p><ul><li>It understands the complexity of most of the stateful applications and have a the corresponding behavior</li><li>The given naming convention is simple to follow: <code>cassandra-X</code></li><li>Coupled with a <code>PodDisruptionBudget</code>, it brings a safetier solution</li><li>With postStart and preStop hooks, I&rsquo;m able to define the tasks I want to address to the Cassandra cluster</li></ul><h2 id=upgrade-stategy>Upgrade stategy</h2><p>Regarding the rollout and <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies>rolling upgrade/restart strategy</a>, it&rsquo;s really simple with this kind of rules on a Statefulset, I&rsquo;m sure the procedure won&rsquo;t go too fast:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>updateStrategy</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>type</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;RollingUpdate&#34;</span></code></pre></div><h2 id=pod-disruption-budget>Pod disruption budget</h2><p>Using the <code>PodDisruptionBudget</code>, I&rsquo;m also sure I won&rsquo;t have more than 1 (the default) planned node down in my cluster. With a RF of 3, I know I can loose a node during a rolling restart:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>maxUnavailable</span><span class=p>:</span><span class=w> </span>{{<span class=w> </span>.Values.cassandraMaxUnavailableNodes<span class=w> </span>}}<span class=w>
</span><span class=w></span><span class=k>selector</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>app</span><span class=p>:</span><span class=w> </span>{{<span class=w> </span>template<span class=w> </span><span class=s2>&#34;kubernetes.name&#34;</span><span class=w> </span>.<span class=w> </span>}}</code></pre></div><h2 id=run-override>Run override</h2><p>I also added an override script to replace run.sh for several reasons:</p><ul><li><strong>Prerequisites</strong>: I wanted to set some global variables first requiring shell and pipes stuffs.</li><li><strong>Flexibility</strong>: it makes possible to add specific command before the instance start. This could be very be useful if you need to run at a given time some commands, but can&rsquo;t rolling restart the whole cluster. This already saved my life twice.</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=cp>#!/bin/bash
</span><span class=cp></span>
<span class=nb>source</span> /usr/local/apache-cassandra/scripts/envVars.sh
/usr/local/apache-cassandra/scripts/jvm_options.sh

/run.sh</code></pre></div><h2 id=post-start-hook>Post start hook</h2><p>In the pre start hook:</p><ul><li>I&rsquo;m ensuring the service is up and running</li><li>I&rsquo;m registering the cluster to <a href=http://cassandra-reaper.io>Cassandra reaper</a> if not already done (will talk later about Cassandra Reaper)</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=cp>#!/bin/bash
</span><span class=cp></span><span class=nb>source</span> /usr/local/apache-cassandra/scripts/envVars.sh

<span class=k>until</span> /ready-probe.sh <span class=p>;</span> <span class=k>do</span>
  <span class=nb>echo</span> <span class=s2>&#34;Waiting node to be ready&#34;</span>
  sleep <span class=m>1</span>
<span class=k>done</span>

<span class=o>{{</span>- <span class=k>if</span> .Values.cassandraReaper.enabled <span class=o>}}</span>
<span class=c1># Wait until replicas is 3 to ready Cassandra Reaper database</span>
<span class=k>if</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=k>$(</span>hostname<span class=k>)</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s1>&#39;cassandra-2&#39;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
  cqlsh -e <span class=s2>&#34;CREATE KEYSPACE IF NOT EXISTS reaper_db WITH replication = {&#39;class&#39;: &#39;NetworkTopologyStrategy&#39;, &#39;{{ .Values.cassandraDC }}&#39;: 3};&#34;</span> &gt; /var/log/reaperdb.log
<span class=k>fi</span>
<span class=o>{{</span>- end <span class=o>}}</span>

<span class=o>{{</span>- <span class=k>if</span> .Values.cassandraReaperRegister.enableReaperRegister <span class=o>}}</span>
<span class=c1># Register cluster into Cassandra Reaper</span>
<span class=c1># Do not target current instance to avoid unready instance status (and registration failure)</span>
<span class=k>if</span> <span class=o>[</span> <span class=s2>&#34;</span><span class=k>$(</span>hostname<span class=k>)</span><span class=s2>&#34;</span> <span class=o>==</span> <span class=s1>&#39;cassandra-0&#39;</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
  <span class=nv>seedHost</span><span class=o>=</span><span class=k>$(</span>hostname -A <span class=p>|</span> sed <span class=s1>&#39;s/^cassandra-0/cassandra-1/&#39;</span><span class=k>)</span>
<span class=k>else</span>
  <span class=nv>seedHost</span><span class=o>=</span><span class=k>$(</span>hostname -A <span class=p>|</span> sed -r <span class=s1>&#39;s/^cassandra-\w+(\..+)/cassandra-0\1/&#39;</span><span class=k>)</span>
<span class=k>fi</span>

<span class=c1># Check if cluster needs to be registred or simply updated</span>
<span class=nv>counter</span><span class=o>=</span><span class=m>0</span>
<span class=k>if</span> <span class=o>[</span> <span class=k>$(</span>curl -s <span class=s2>&#34;http://{{ .Values.cassandraReaperRegister.reaperServerServiceName }}/cluster&#34;</span> <span class=p>|</span> grep -c <span class=nv>$CASSANDRA_CLUSTER_NAME</span><span class=k>)</span> <span class=o>==</span> <span class=m>0</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
  <span class=k>while</span> <span class=o>[</span> <span class=k>$(</span>curl -s -I -X POST <span class=s2>&#34;http://{{ .Values.cassandraReaperRegister.reaperServerServiceName }}/cluster?seedHost=</span><span class=nv>$seedHost</span><span class=s2>&#34;</span> <span class=p>|</span> grep -c <span class=s2>&#34;^HTTP/1.1 201 Created&#34;</span><span class=k>)</span> !<span class=o>=</span> <span class=m>1</span> <span class=o>]</span> <span class=p>;</span> <span class=k>do</span>
    <span class=k>if</span> <span class=o>[</span> <span class=nv>$counter</span> <span class=o>==</span> <span class=m>5</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
      <span class=nb>break</span>
    <span class=k>fi</span>
    <span class=nv>counter</span><span class=o>=</span><span class=k>$((</span>counter+1<span class=k>))</span>
    sleep <span class=m>5</span>
  <span class=k>done</span>
<span class=k>else</span>
  <span class=k>while</span> <span class=o>[</span> <span class=k>$(</span>curl -s -I -X PUT <span class=s2>&#34;http://{{ .Values.cassandraReaperRegister.reaperServerServiceName }}/cluster/</span><span class=si>${</span><span class=nv>CASSANDRA_CLUSTER_NAME</span><span class=si>}</span><span class=s2>?seedHost=</span><span class=nv>$seedHost</span><span class=s2>&#34;</span> <span class=p>|</span> egrep -c <span class=s2>&#34;^HTTP/1.1 (200|304)&#34;</span><span class=k>)</span> !<span class=o>=</span> <span class=m>1</span> <span class=o>]</span> <span class=p>;</span> <span class=k>do</span>
    <span class=k>if</span> <span class=o>[</span> <span class=nv>$counter</span> <span class=o>==</span> <span class=m>5</span> <span class=o>]</span> <span class=p>;</span> <span class=k>then</span>
      <span class=nb>break</span>
    <span class=k>fi</span>
    <span class=nv>counter</span><span class=o>=</span><span class=k>$((</span>counter+1<span class=k>))</span>
    sleep <span class=m>5</span>
  <span class=k>done</span>
<span class=k>fi</span>
<span class=o>{{</span>- end <span class=o>}}</span>
<span class=nb>exit</span> <span class=m>0</span></code></pre></div><h2 id=pre-stop-hook>Pre stop hook</h2><p>In the pre stop hook:</p><ul><li>I&rsquo;m ensure the correct health of the cluster before requesting node leave. This to avoid a total blackout</li><li>I&rsquo;m ensuring the node stops properly</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=cp>#!/bin/sh
</span><span class=cp></span>
run_nodetool<span class=o>()</span> <span class=o>{</span>
  <span class=nb>echo</span> <span class=s2>&#34;Running: nodetool </span><span class=nv>$1</span><span class=s2>&#34;</span>
  /usr/local/apache-cassandra/bin/nodetool <span class=nv>$1</span>
  sleep <span class=m>5</span>
<span class=o>}</span>

<span class=k>while</span> <span class=o>[</span> <span class=k>$(</span>/usr/local/apache-cassandra/bin/nodetool status <span class=p>|</span> awk <span class=s2>&#34;/</span><span class=nv>$CASSANDRA_RACK</span><span class=s2>/{ print \$1,\$2 }&#34;</span> <span class=p>|</span> grep -v <span class=nv>$POD_IP</span> <span class=p>|</span> awk <span class=s1>&#39;{ print $1 }&#39;</span> <span class=p>|</span> grep -v UN<span class=k>)</span> -eq <span class=m>0</span> <span class=o>]</span> <span class=p>;</span> <span class=k>do</span>
  <span class=nb>echo</span> <span class=s2>&#34;Waiting all nodes to recover a correct status before draining this node&#34;</span>
  sleep <span class=m>5</span>
  pidof java <span class=o>||</span> <span class=nb>exit</span> <span class=m>1</span>
<span class=k>done</span>

run_nodetool disablethrift
run_nodetool disablebinary
run_nodetool disablegossip
run_nodetool flush
run_nodetool drain
sleep <span class=m>10</span>
run_nodetool stop
run_nodetool stopdaemon

<span class=nb>exit</span> <span class=m>0</span></code></pre></div><h1 id=observability>Observability</h1><p>Observing a Cassandra cluster may not be an easy task. There are plenty of metrics per nodes which can confuse the beginners. That&rsquo;s why providing an exporter with a pre-made Grafana dashboard is a good solution for everyone.</p><h2 id=cassandra-exporter>Cassandra exporter</h2><p>To make it simple, I&rsquo;m using the Prometheus format (as it becomes the standard now) and I&rsquo;m using <a href=https://github.com/criteo/cassandra_exporter>Cassandra Exporter</a> (made by my NoSQL team at <a href=https://www.criteo.com>Criteo</a>).</p><p>On each nodes it will deploy an exporter (an additional container in the Cassandra pod):</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=c># Cassandra Exporter</span><span class=w>
</span><span class=w></span><span class=k>cassandraExporter</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>enableExporter</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>  </span><span class=k>replicaCount</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=k>imageVersion</span><span class=p>:</span><span class=w> </span><span class=m>1.0.2</span><span class=w>
</span><span class=w>  </span><span class=k>nodeSelector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>node-role.kubernetes.io/node</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;true&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>resources</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>limits</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>cpu</span><span class=p>:</span><span class=w> </span>300m<span class=w>
</span><span class=w>      </span><span class=k>memory</span><span class=p>:</span><span class=w> </span>500Mi<span class=w>
</span><span class=w>    </span><span class=k>requests</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>cpu</span><span class=p>:</span><span class=w> </span>300m<span class=w>
</span><span class=w>      </span><span class=k>memory</span><span class=p>:</span><span class=w> </span>500Mi<span class=w>
</span><span class=w>  </span><span class=k>config</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>host</span><span class=p>:</span><span class=w> </span><span class=m>127.0.0.1</span><span class=p>:</span><span class=m>7199</span><span class=w>
</span><span class=w>    </span><span class=k>listenPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>    </span><span class=k>jvmOpts</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;&#34;</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=c># Prometheus scraping</span><span class=w>
</span><span class=w></span><span class=k>cassandraPrometheusScrap</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>enableScrap</span><span class=p>:</span><span class=w> </span><span class=kc>false</span></code></pre></div><p>If you&rsquo;re using <a href=https://github.com/coreos/prometheus-operator>Prometheus Operator</a>, you can enable automatic scraping here (mean that Prometheus server will automatically retrieve information from exporter).</p><h2 id=grafana>Grafana</h2><p>A Cassandra dashboard compatible with Kubernetes is also available <a href=https://grafana.com/dashboards/6258>here</a>.</p><p><img src=https://blog.deimos.fr/images/cassandra_grafana.png alt=CassandraDashboard></p><p>You only have to import it in your Grafana interface.</p><h2 id=alerting>Alerting</h2><p>Once again, if you&rsquo;re using <a href=https://github.com/coreos/prometheus-operator>Prometheus Operator</a>, you can enable basic alerting rules with Alert Manager:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>cassandraAlertmanager</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>enableAlerts</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=k>alertLabels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>sloInterrupt</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>serviceLevel</span><span class=p>:</span><span class=w> </span>objective<span class=w>
</span><span class=w>      </span><span class=k>severity</span><span class=p>:</span><span class=w> </span>interrupt<span class=w>
</span><span class=w>      </span><span class=k>team</span><span class=p>:</span><span class=w> </span>infra<span class=w>
</span><span class=w>      </span><span class=k>type</span><span class=p>:</span><span class=w> </span>functional<span class=w>
</span><span class=w>    </span><span class=k>sloPage</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>serviceLevel</span><span class=p>:</span><span class=w> </span>objective<span class=w>
</span><span class=w>      </span><span class=k>severity</span><span class=p>:</span><span class=w> </span>page<span class=w>
</span><span class=w>      </span><span class=k>team</span><span class=p>:</span><span class=w> </span>infra<span class=w>
</span><span class=w>      </span><span class=k>type</span><span class=p>:</span><span class=w> </span>functional</code></pre></div><h1 id=maintenance-and-operations>Maintenance and operations</h1><p>On Cassandra, there are 2 others important actions:</p><ul><li><strong>Backups</strong>: as usual, it&rsquo;s always preferable to get backups (just in case)</li><li><strong>Repair</strong>: repair tables to ensure consistency</li></ul><h2 id=backups--restore>Backups / Restore</h2><p>Backuping Cassandra is not so trivial. That&rsquo;s why I&rsquo;ve made some scripts included in the helm chart <strong>ready to plan backups and help on restore</strong>.</p><p><strong>The backups scripts are made to store on AWS S3</strong>. A cronjob exist for it, you simple have to set the &ldquo;cassandraBackup&rdquo; parameters.</p><h3 id=enbale-backups>Enbale backups</h3><p>To enable backups, you simply have to update this section with your options. A Cron job will be created to perform backups of all nodes at the same time (for consistency purpose):</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>cassandraBackup</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>enableBackups</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=k>backupSchedule</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;0 3 * * *&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>backupImageVersion</span><span class=p>:</span><span class=w> </span>v0<span class=m>.1</span><span class=w>
</span><span class=w>  </span><span class=k>awsAccessKeyId</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;xxx&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>awsSecretAccessKey</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;yyy&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>awsPassphrase</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;zzz&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>awsBucket</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;bucket_name&#34;</span><span class=w>
</span><span class=w>  </span><span class=c>#duplicityOptions: &#34;--archive-dir /var/lib/cassandra/.duplicity --allow-source-mismatch --s3-european-buckets --s3-use-new-style --copy-links --num-retries 3 --s3-use-multiprocessing --s3-multipart-chunk-size 100 --volsize 1024 full . s3://s3.amazonaws.com/cassandra-backups/$CLUSTER_DOMAIN/$CASSANDRA_CLUSTER_NAME/$(hostname)&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>awsDestinationPath</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;s3://s3-eu-west-1.amazonaws.com/${AWS_BUCKET}/$CLUSTER_DOMAIN/$CASSANDRA_CLUSTER_NAME/$(hostname)&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>restoreFolder</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/var/lib/cassandra/restore&#34;</span></code></pre></div><h3 id=list>List</h3><p>To list backups for a statefulset instance, connect to an instance like in this example and call the script:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl <span class=nb>exec</span> -it cassandra-0 bash
/usr/local/apache-cassandra/scripts/snapshot2s3.sh list &lt;AWS_ACCESS_KEY_ID&gt; &lt;AWS_SECRET_ACCESS_KEY&gt; &lt;AWS_PASSPHRASE&gt; &lt;AWS_BUCKET&gt;</code></pre></div><p>Replace all AWS information with the corresponding ones.</p><h3 id=restore>Restore</h3><p>To restore backups, configure properly the &ldquo;restoreFolder&rdquo; var and run this command:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl <span class=nb>exec</span> -it cassandra-0 bash
/usr/local/apache-cassandra/scripts/snapshot2s3.sh restore &lt;AWS_ACCESS_KEY_ID&gt; &lt;AWS_SECRET_ACCESS_KEY&gt; &lt;AWS_PASSPHRASE&gt; &lt;AWS_BUCKET&gt; &lt;RESTORE_TIME&gt;</code></pre></div><p>RESTORE_TIME should be used as described in the <a href=http://duplicity.nongnu.org/duplicity.1.html#sect8>Duplicity manual Time Format section</a>.
For example (3D to restore the the last 3 days backup)</p><p>You can then use the script cassandra-restore.sh to restore a desired keyspace with all or one table:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>/usr/local/apache-cassandra/scripts/cassandra-restore.sh /var/lib/cassandra/restore/var/lib/cassandra/data <span class=o>[</span>keyspace<span class=o>]</span></code></pre></div><p>Try to use help if you need more info about it.</p><h2 id=repair-tables>Repair tables</h2><p>Repairing tables may be painfull sometimes. Hopefully <a href=http://cassandra-reaper.io>Cassandra Reaper</a> helps to avoid managing it manually.</p><p>You can deploy a server instance (require a Cassandra backend):</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>cassandraReaper</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>enableReaper</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=k>replicaCount</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=k>imageVersion</span><span class=p>:</span><span class=w> </span><span class=m>1.1.0</span><span class=w>
</span><span class=w>  </span><span class=k>nodeSelector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>node-role.kubernetes.io/node</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;true&#34;</span><span class=w>
</span><span class=w>  </span><span class=k>contactPoints</span><span class=p>:</span><span class=w> </span>cassandra<span class=m>-0.</span>cassandra<span class=w>
</span><span class=w>  </span><span class=k>jmxAuth</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>username</span><span class=p>:</span><span class=w> </span>reaperUser<span class=w>
</span><span class=w>    </span><span class=k>password</span><span class=p>:</span><span class=w> </span>reaperPass<span class=w>
</span><span class=w>  </span><span class=c># clusterName:</span><span class=w>
</span><span class=w>  </span><span class=c># keyspace: reaper_db</span><span class=w>
</span><span class=w>  </span><span class=k>envVariables</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>JAVA_OPTS</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 -XX:+AlwaysPreTouch&#34;</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_CASS_KEYSPACE</span><span class=p>:</span><span class=w> </span>reaper_db<span class=w>
</span><span class=w>    </span><span class=k>REAPER_REPAIR_PARALELLISM</span><span class=p>:</span><span class=w> </span>DATACENTER_AWARE<span class=w>
</span><span class=w>    </span><span class=k>REAPER_REPAIR_INTENSITY</span><span class=p>:</span><span class=w> </span><span class=m>0.5</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_AUTO_SCHEDULING_ENABLED</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_SCHEDULE_DAYS_BETWEEN</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_REPAIR_RUN_THREADS</span><span class=p>:</span><span class=w> </span><span class=m>16</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_HANGING_REPAIR_TIMEOUT_MINS</span><span class=p>:</span><span class=w> </span><span class=m>30</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_REPAIR_MANAGER_SCHEDULING_INTERVAL_SECONDS</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_SEGMENT_COUNT</span><span class=p>:</span><span class=w> </span><span class=m>200</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_LOGGING_ROOT_LEVEL</span><span class=p>:</span><span class=w> </span>INFO<span class=w>
</span><span class=w>    </span><span class=k>REAPER_SERVER_ADMIN_PORT</span><span class=p>:</span><span class=w> </span><span class=m>8081</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_SERVER_APP_PORT</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>    </span><span class=k>REAPER_METRICS_ENABLED</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=k>resources</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=k>limits</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>cpu</span><span class=p>:</span><span class=w> </span>500m<span class=w>
</span><span class=w>      </span><span class=k>memory</span><span class=p>:</span><span class=w> </span>500Mi<span class=w>
</span><span class=w>    </span><span class=k>requests</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=k>cpu</span><span class=p>:</span><span class=w> </span>500m<span class=w>
</span><span class=w>      </span><span class=k>memory</span><span class=p>:</span><span class=w> </span>500Mi</code></pre></div><p>And then you simply have enable client registration to get all your nodes and cluster automatically added into Cassandra Reaper:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=k>cassandraReaperRegister</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=k>enableReaperRegister</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=k>reaperServerServiceName</span><span class=p>:</span><span class=w> </span>cassandra-reaper.svc</code></pre></div><p>Finally you&rsquo;ll find the reaper interface available:</p><p><img src=https://blog.deimos.fr/images/cassandra_reaper.jpg alt=CassandraReaper></p><h1 id=conclusion>Conclusion</h1><p>This <a href=https://github.com/MySocialApp/kubernetes-helm-chart-cassandra/>Cassandra helm chart</a> is helping me day to day. It gives a lot of autonomy to my cluster, gives me the possibility to switch to fallback to a manual way to manage specific Cassandra cases, gives enough automation to avoid spending too many time on managing my Cassandra clusters. I&rsquo;m pretty well satisfied of this solution.</p><p>Some of you may think an Operator would be preferable, that&rsquo;s true. However in nowdays, there is no stable enough Operator managing Cassandra. Writing one takes some time and I unfortunately do not get this time today (but be happy to participate if a serious initiative is driven) and getting one ready to handle every situations will require a ton of tests and time.</p><p>So feel free to participate and I hope you&rsquo;ll enjoy it :)</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">TAGGED IN</span><br><a class="tag tag--primary tag--small" href=https://blog.deimos.fr/tags/kubernetes/>Kubernetes</a>
<a class="tag tag--primary tag--small" href=https://blog.deimos.fr/tags/containers/>Containers</a>
<a class="tag tag--primary tag--small" href=https://blog.deimos.fr/tags/cassandra/>Cassandra</a>
<a class="tag tag--primary tag--small" href=https://blog.deimos.fr/tags/databases/>Databases</a>
<a class="tag tag--primary tag--small" href=https://blog.deimos.fr/tags/distributed/>Distributed</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.deimos.fr/2018/10/11/converting-raw-to-jpg-with-nice-rendering/ data-tooltip="Converting RAW to JPG with nice rendering" aria-label="NEXT: Converting RAW to JPG with nice rendering"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">NEXT</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.deimos.fr/2018/01/31/integrate-gcr-in-your-on-premise-kubernetes/ data-tooltip="Integrate GCR in your on premise k8s cluster" aria-label="PREVIOUS: Integrate GCR in your on premise k8s cluster"><span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions aria-label="Share this post"><i class="fa fa-share-alt" aria-hidden=true></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/" title="Share on Facebook" aria-label="Share on Facebook"><i class=fa-facebook-official aria-hidden=true></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/" title="Share on Twitter" aria-label="Share on Twitter"><i class=fa-twitter aria-hidden=true></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#top aria-label="Back to top"><i class="fa fa-arrow-up" aria-hidden=true></i></a></li></ul></div></div></article><footer id=footer class=main-content-wrap><span class=copyrights>&copy; 2021 Pierre Mavro / Deimosfr. All Rights Reserved</span></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.deimos.fr/2018/10/11/converting-raw-to-jpg-with-nice-rendering/ data-tooltip="Converting RAW to JPG with nice rendering" aria-label="NEXT: Converting RAW to JPG with nice rendering"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">NEXT</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://blog.deimos.fr/2018/01/31/integrate-gcr-in-your-on-premise-kubernetes/ data-tooltip="Integrate GCR in your on premise k8s cluster" aria-label="PREVIOUS: Integrate GCR in your on premise k8s cluster"><span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions aria-label="Share this post"><i class="fa fa-share-alt" aria-hidden=true></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/" title="Share on Facebook" aria-label="Share on Facebook"><i class=fa-facebook-official aria-hidden=true></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://blog.deimos.fr/2018/06/24/running-cassandra-on-kubernetes/" title="Share on Twitter" aria-label="Share on Twitter"><i class=fa-twitter aria-hidden=true></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#top aria-label="Back to top"><i class="fa fa-arrow-up" aria-hidden=true></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-times"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fblog.deimos.fr%2F2018%2F06%2F24%2Frunning-cassandra-on-kubernetes%2F" aria-label="Share on Facebook"><i class=fa-facebook-official aria-hidden=true></i><span>Share on Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fblog.deimos.fr%2F2018%2F06%2F24%2Frunning-cassandra-on-kubernetes%2F" aria-label="Share on Twitter"><i class=fa-twitter aria-hidden=true></i><span>Share on Twitter</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-times"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/194382a6478d109ac45c9b11bad1945e?s=110" alt="Author's picture"><h4 id=about-card-name>Pierre Mavro / Deimosfr</h4><div id=about-card-job><i class="fa fa-briefcase"></i><br>Qovery Co-Founder and CTO</div><div id=about-card-location><i class="fa fa-map-marker-alt"></i><br>Paris - France</div></div></div><div id=cover style=background-image:url(https://blog.deimos.fr/images/fish_background.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/highlight.min.js integrity="sha512-z+/WWfyD5tccCukM4VvONpEtLmbAm5LDu7eKiyMQJ9m7OfPEDL7gENyDRL3Yfe8XAuGsS2fS4xSMnl6d30kqGQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://blog.deimos.fr/js/script-yqzy9wdlzix4lbbwdnzvwx3egsne77earqmn73v9uno8aupuph8wfguccut.min.js></script><script src=https://blog.deimos.fr/js/custom.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight > code, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script></body></html>