<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ceph on Deimosfr Blog</title>
    <link>https://blog.deimos.fr/tags/ceph/</link>
    <description>Recent content in Ceph on Deimosfr Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Oct 2014 10:00:30 +0000</lastBuildDate><atom:link href="https://blog.deimos.fr/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>FIO: Bench IO disks</title>
      <link>https://blog.deimos.fr/2014/10/06/fio-bench-io-disks/</link>
      <pubDate>Mon, 06 Oct 2014 10:00:30 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/10/06/fio-bench-io-disks/</guid>
      <description>The first thing you generally want to do when you have any new Storage system like SSD, Disk arrays or a Cluster Ceph, is benching. You will want to know how can read and write throughput. FIO is able to do that for you, here is an example:
[global] ioengine=libaio invalidate=1 ramp_time=5 direct=1 size=5G runtime=300 time_based directory=/home [seq-read] rw=read bs=64K stonewall [rand-read] rw=randread bs=4K stonewall [seq-write] rw=write bs=64K stonewall [rand-write] rw=randwrite bs=4K stonewall You then will have a good output of everything you need to know.</description>
    </item>
    
    <item>
      <title>Openstack Meetup 9: Ceph, Swift and ZFS!</title>
      <link>https://blog.deimos.fr/2014/06/14/openstack-meetup-9-ceph-swift-and-zfs/</link>
      <pubDate>Sat, 14 Jun 2014 10:00:40 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/06/14/openstack-meetup-9-ceph-swift-and-zfs/</guid>
      <description>Do not miss OpenStack meetup event at eNovance headquarter! There will be big talks on:
 ZFS Ceph Swift  You&amp;rsquo;ve understood, it&#39; storage related for OpenStack.
Of course I&amp;rsquo;ll be there :-D. More informations are available here.</description>
    </item>
    
    <item>
      <title>Ceph: cluster GUI is now OpenSource!!! Thanks RedHat</title>
      <link>https://blog.deimos.fr/2014/06/04/ceph-calamari-gui-is-now-opensource-thanks-redhat/</link>
      <pubDate>Wed, 04 Jun 2014 10:00:41 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/06/04/ceph-calamari-gui-is-now-opensource-thanks-redhat/</guid>
      <description>This is a very good news if you&amp;rsquo;re using Ceph in production! Originally delivered as a proprietary dashboard included with Inktank Ceph Enterprise, Calamari has some really great visualization stuff for your cluster as well as the long term goal of being the all-in-wonder management system that can configure and analyze a Ceph cluster.
Calamari is composed in 2 elements:
 Backend: the Calamari backend is written in Python 2.6+, using Saltstack, ZeroRPC, gevent, Django, django-rest-framework, graphite, (and a few others I may have forgotten) and instantiates a new REST API for integration with other systems.</description>
    </item>
    
    <item>
      <title>Ceph: why don’t you try Ceph and stop disks array?</title>
      <link>https://blog.deimos.fr/2014/04/11/ceph-why-dont-you-try-ceph-and-stop-disks-array/</link>
      <pubDate>Fri, 11 Apr 2014 10:00:00 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/04/11/ceph-why-dont-you-try-ceph-and-stop-disks-array/</guid>
      <description>In France, we have the chance to have an ISP/Mobile Telephony provider called Free. They considerably reduced the market price for the Mobile phone communications in 2012. This was very fine for competition and for French people. Now we can have correct prices for Internet and cell phone data. Thanks to Free.
Ceph is quite the same as Free but for storage systems. Since many years, disks array constructors sold us locked solutions with a very high cost of hardware and licenses&#39; software.</description>
    </item>
    
    <item>
      <title>Easily deploy Ceph Cluster with Ansible</title>
      <link>https://blog.deimos.fr/2014/03/06/ceph-deploy-it-with-ansible/</link>
      <pubDate>Thu, 06 Mar 2014 11:00:00 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/03/06/ceph-deploy-it-with-ansible/</guid>
      <description>One of my colleague (Sebastien Han) started to play (like me) with Ansible. As he is a Ceph master, he often deploy Ceph for tests or production. He generally uses Puppet for that purpose.
But due to some Puppet limitations or big brain fuck it involves, he started to play with Ansible and created a Ceph module for it. He showed me it, when he was writing it and it seamed working like a charm.</description>
    </item>
    
    <item>
      <title>Ceph Day in Frankfurt next week !</title>
      <link>https://blog.deimos.fr/2014/02/22/ceph-day-in-frankfurt-next-week/</link>
      <pubDate>Sat, 22 Feb 2014 11:00:48 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/02/22/ceph-day-in-frankfurt-next-week/</guid>
      <description>I&amp;rsquo;ll be once again happy to participate to Ceph Day ! We&amp;rsquo;ll see the new version of Ceph, best practices and how to scale it right :-)
This sounds good ! To get more informations: https://www.eventbrite.com/e/ceph-day-frankfurt-tickets-10173269523
Thanks for eNovance to send me there, I&amp;rsquo;m really happy !</description>
    </item>
    
    <item>
      <title>Ceph Meetup: What’s new in Firefly ?</title>
      <link>https://blog.deimos.fr/2014/02/14/ceph-meetup-whats-new-in-firefly/</link>
      <pubDate>Fri, 14 Feb 2014 11:00:44 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/02/14/ceph-meetup-whats-new-in-firefly/</guid>
      <description>Don&amp;rsquo;t miss the Ceph Meetup on Monday ! You will be aware of the new features in Firefly (like tiering or erasure coding) ! And this is the first long time support release !!!
To get more informations: http://www.meetup.com/Ceph-in-Paris/events/158942372/</description>
    </item>
    
    <item>
      <title>Ceph: setup Rados Block Devices (RBD) nodes on Debian</title>
      <link>https://blog.deimos.fr/2014/01/21/ceph-setup-block-devices-rbd-nodes-on-debian/</link>
      <pubDate>Tue, 21 Jan 2014 11:00:00 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/01/21/ceph-setup-block-devices-rbd-nodes-on-debian/</guid>
      <description>In addition of object storage, Ceph is able to provide block devices. Ceph’s RADOS Block Device (RBD) provides access to block device images that are striped and replicated across the entire storage cluster.
For example this is used to store Virtual Machines on OpenStack. With that solution, you&amp;rsquo;ve got a real fault tolerance system for your VM and distributed.
The RBD part is an easy task when you&amp;rsquo;ve already setup a Ceph cluster.</description>
    </item>
    
    <item>
      <title>Ceph: setup Objects Storage (OSD) nodes on Debian</title>
      <link>https://blog.deimos.fr/2014/01/15/ceph-setup-objects-storage-osd-nodes-on-debian/</link>
      <pubDate>Wed, 15 Jan 2014 11:00:51 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/01/15/ceph-setup-objects-storage-osd-nodes-on-debian/</guid>
      <description>Following the previous post, on Ceph Storage, I&amp;rsquo;ve updated my documentation with informations to play with OSD. I&amp;rsquo;ve covered :
 Setup OSD Remove OSD Use Ceph Ojects Storage  Using the object storage system is not complicated and easily scalable. It&amp;rsquo;s a real pleasure to work with that kind of solutions. If you&amp;rsquo;re already working with other objects storage solutions, you won&amp;rsquo;t be lost.</description>
    </item>
    
    <item>
      <title>Ceph: setup Monitors nodes (mon) on Debian</title>
      <link>https://blog.deimos.fr/2014/01/13/ceph-installation-and-configuration-on-debian/</link>
      <pubDate>Mon, 13 Jan 2014 11:00:12 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2014/01/13/ceph-installation-and-configuration-on-debian/</guid>
      <description>I finally started to play with Ceph to add it in production. I&amp;rsquo;m still impressed by this storage solution. Just to remind, Ceph is an open-source, massively scalable, software-defined storage system which provides object, block and file system storage (not yet ready for production usage) in a single platform. It runs on commodity hardware-saving you costs, giving you flexibility. And as it’s in the Linux kernel, it’s easy to consume.</description>
    </item>
    
    <item>
      <title>Ceph Day in London : here we go !</title>
      <link>https://blog.deimos.fr/2013/10/09/ceph-day-in-london-here-we-go/</link>
      <pubDate>Wed, 09 Oct 2013 10:00:43 +0000</pubDate>
      
      <guid>https://blog.deimos.fr/2013/10/09/ceph-day-in-london-here-we-go/</guid>
      <description>Ceph is a high availability way to store your data. I really love that solution and how it works. It&amp;rsquo;s a real good alternative to high cost disks storage. With some colleges of eNovance, we&amp;rsquo;re currently participating to the Ceph Day today in London (http://www.inktank.com/CEPHdays/).
I&amp;rsquo;m very excited to be here, as I wanted to put in production Ceph, 4-5 years ago (when it was released as experimental in Linux Kernel).</description>
    </item>
    
  </channel>
</rss>
